# -*- coding: utf-8 -*-
"""Job Posting Classification .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fJ8enmHUKv6OEthmTkTcjNnI9xgEa-fJ
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

def scrape_karkidi_jobs(keyword="data science", pages=1):
    headers = {'User-Agent': 'Mozilla/5.0'}
    base_url = "https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}"
    jobs_list = []

    for page in range(1, pages + 1):
        url = base_url.format(page=page, query=keyword.replace(' ', '%20'))
        print(f"Scraping page: {page}")
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.content, "html.parser")

        job_blocks = soup.find_all("div", class_="ads-details")
        for job in job_blocks:
            try:
                title = job.find("h4").get_text(strip=True)
                company = job.find("a", href=lambda x: x and "Employer-Profile" in x).get_text(strip=True)
                location = job.find("p").get_text(strip=True)
                experience = job.find("p", class_="emp-exp").get_text(strip=True)
                key_skills_tag = job.find("span", string="Key Skills")
                skills = key_skills_tag.find_next("p").get_text(strip=True) if key_skills_tag else ""
                summary_tag = job.find("span", string="Summary")
                summary = summary_tag.find_next("p").get_text(strip=True) if summary_tag else ""

                jobs_list.append({
                    "Title": title,
                    "Company": company,
                    "Location": location,
                    "Experience": experience,
                    "Summary": summary,
                    "Skills": skills
                })
            except Exception as e:
                print(f"Error parsing job block: {e}")
                continue

        time.sleep(1)  # Be nice to the server

    return pd.DataFrame(jobs_list)

# Example use:
if __name__ == "__main__":
    df_jobs = scrape_karkidi_jobs(keyword="data science", pages=2)
    print(df_jobs.head())

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import AgglomerativeClustering
from sklearn.neighbors import NearestCentroid
import pandas as pd
import joblib

# Define the tokenizer globally
def split_skills_tokenizer(x):
    """Custom tokenizer to split skills by comma."""
    return x.split(',')

def preprocess_skills(df):
    df = df.copy()
    df['Skills'] = df['Skills'].fillna("").str.lower().str.strip()
    return df

def vectorize_skills(df):
    vectorizer = TfidfVectorizer(tokenizer=split_skills_tokenizer, lowercase=True)
    X = vectorizer.fit_transform(df['Skills'])
    return X, vectorizer

def cluster_skills(X, n_clusters=5):
    model = AgglomerativeClustering(n_clusters=n_clusters)
    labels = model.fit_predict(X.toarray())  # Note: must convert sparse to dense
    return model, labels

def train_centroid_classifier(X, labels):
    clf = NearestCentroid()
    clf.fit(X.toarray(), labels)
    return clf

def classify_new_jobs(df_new, vectorizer, clf):
    df_new = preprocess_skills(df_new)
    X_new = vectorizer.transform(df_new['Skills'])
    df_new['Cluster'] = clf.predict(X_new.toarray())
    return df_new

def notify_user(df_classified, user_cluster_id):
    matched = df_classified[df_classified['Cluster'] == user_cluster_id]
    if not matched.empty:
        print("üîî New job(s) matching your interest:")
        display(matched[['Title', 'Company', 'Skills']])
    else:
        print("‚ùå No new matching jobs today.")

# === PIPELINE ===

# Step 1: Scrape initial jobs
df_jobs = scrape_karkidi_jobs("data science", pages=5)

# Step 2: Preprocess
df_jobs = preprocess_skills(df_jobs)

# Step 3: Vectorize and Cluster
X, vectorizer = vectorize_skills(df_jobs)
model, labels = cluster_skills(X, n_clusters=5)
df_jobs['Cluster'] = labels

# Step 4: Train a centroid classifier for future prediction
clf = train_centroid_classifier(X, labels)

# Step 5: Simulate new jobs
df_new_jobs = scrape_karkidi_jobs("data science", pages=1)

# Step 6: Predict new job clusters
df_classified = classify_new_jobs(df_new_jobs, vectorizer, clf)

# Step 7: Notify user (assume interest in cluster 2)
notify_user(df_classified, user_cluster_id=2)

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import AgglomerativeClustering
from sklearn.neighbors import NearestCentroid
import joblib

# === Scraper: scrape job listings from karkidi.com ===
def scrape_karkidi_jobs(keyword="data science", pages=1):
    headers = {'User-Agent': 'Mozilla/5.0'}
    base_url = "https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}"
    jobs_list = []

    for page in range(1, pages + 1):
        url = base_url.format(page=page, query=keyword.replace(' ', '%20'))
        print(f"Scraping page: {page}")
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.content, "html.parser")

        # Find all job posting blocks on the page
        job_blocks = soup.find_all("div", class_="ads-details")
        for job in job_blocks:
            try:
                # Extract job title, company, location, experience
                title = job.find("h4").get_text(strip=True)
                company = job.find("a", href=lambda x: x and "Employer-Profile" in x).get_text(strip=True)
                location = job.find("p").get_text(strip=True)
                experience = job.find("p", class_="emp-exp").get_text(strip=True)

                # Extract "Key Skills" and "Summary" sections if present
                spans = job.find_all("span")
                skills = ""
                summary = ""
                for span in spans:
                    text = span.get_text(strip=True).lower()
                    if text == "key skills":
                        skills_tag = span.find_next("p")
                        if skills_tag:
                            skills = skills_tag.get_text(strip=True)
                    elif text == "summary":
                        summary_tag = span.find_next("p")
                        if summary_tag:
                            summary = summary_tag.get_text(strip=True)

                # Append extracted info as a dict
                jobs_list.append({
                    "Title": title,
                    "Company": company,
                    "Location": location,
                    "Experience": experience,
                    "Summary": summary,
                    "Skills": skills
                })
            except Exception as e:
                print(f"Error parsing job block: {e}")
                continue

        time.sleep(1)  # Be polite: wait between requests

    # Convert the list of dicts to a DataFrame
    return pd.DataFrame(jobs_list)

# === Preprocessing: clean and normalize skills text ===
def split_skills_tokenizer(x):
    # Custom tokenizer to split skill strings by commas
    return x.split(',')

def preprocess_skills(df):
    df = df.copy()
    # Lowercase, strip whitespace, fill missing skills with empty string
    df['Skills'] = df['Skills'].fillna("").str.lower().str.strip()
    return df

# === Vectorization and Clustering ===
def vectorize_skills(df):
    # TF-IDF vectorizer using custom tokenizer to handle comma-separated skills
    vectorizer = TfidfVectorizer(tokenizer=split_skills_tokenizer, lowercase=True)
    X = vectorizer.fit_transform(df['Skills'])
    return X, vectorizer

def cluster_skills(X, n_clusters=5):
    # Agglomerative clustering on the dense skill vectors
    model = AgglomerativeClustering(n_clusters=n_clusters)
    labels = model.fit_predict(X.toarray())
    return model, labels

def train_centroid_classifier(X, labels):
    # Train a Nearest Centroid classifier on the cluster labels for quick prediction
    clf = NearestCentroid()
    clf.fit(X.toarray(), labels)
    return clf

# === Classify new jobs and notify user ===
def classify_new_jobs(df_new, vectorizer, clf):
    # Preprocess new job data
    df_new = preprocess_skills(df_new)
    # Vectorize new job skills
    X_new = vectorizer.transform(df_new['Skills'])
    # Predict cluster for each new job
    df_new['Cluster'] = clf.predict(X_new.toarray())
    return df_new

def notify_user(df_classified, user_cluster_id):
    # Filter jobs matching the user's interested cluster
    matched = df_classified[df_classified['Cluster'] == user_cluster_id]
    if not matched.empty:
        print("üîî New job(s) matching your interest:")
        print(matched[['Title', 'Company', 'Skills']])
    else:
        print("‚ùå No new matching jobs today.")

# === Main pipeline ===
if __name__ == "__main__":
    # Step 1: Scrape initial jobs (e.g., 5 pages)
    df_jobs = scrape_karkidi_jobs("data science", pages=5)
    df_jobs = preprocess_skills(df_jobs)

    # Step 2: Vectorize skills
    X, vectorizer = vectorize_skills(df_jobs)

    # Step 3: Check for empty or zero variance vectors before clustering
    if X.shape[0] == 0 or np.all(X.toarray() == 0):
        print("‚ùå No useful skill data found. Exiting.")
        exit()

    # Step 4: Perform clustering and assign cluster labels
    model, labels = cluster_skills(X, n_clusters=5)
    df_jobs['Cluster'] = labels

    # Step 5: Train centroid classifier for fast future predictions
    clf = train_centroid_classifier(X, labels)

    # Step 6: Save vectorizer and classifier models to disk
    joblib.dump(vectorizer, "vectorizer.joblib")
    joblib.dump(clf, "classifier.joblib")

    # Step 7: Simulate new job scraping and classify them
    df_new_jobs = scrape_karkidi_jobs("data science", pages=1)
    df_classified = classify_new_jobs(df_new_jobs, vectorizer, clf)

    # Step 8: Notify user of new jobs in cluster 2 (example)
    notify_user(df_classified, user_cluster_id=2)